{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gurobi and numpy\n",
    "from gurobipy import *\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "# load data\n",
    "train = genfromtxt('LRTrain.csv', delimiter=',', skip_header = 1)\n",
    "test = genfromtxt('LRTest.csv', delimiter=',', skip_header = 1)\n",
    "\n",
    "feat_train = train[:, :-1]\n",
    "label_train = train[:, -1]\n",
    "feat_test = test[:, :-1]\n",
    "label_test = test[:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, I found that the original step size of 0.00001 and max iterations of 2000 yeilded a suboptimal solution. In order to find the optimal solution, I repeatedley decreased epsilon (the termination criterion related to the norm of the gradient) and adjusted gamma (the step size) to ensure convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training logistic regression classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples\n",
    "n = 300\n",
    "# number of features\n",
    "d = 30\n",
    "# step size\n",
    "gamma = 0.00003\n",
    "# set threshold on norm of gradient\n",
    "epsilon = 0.1\n",
    "\n",
    "# set initial weights\n",
    "w = np.zeros(d)\n",
    "# initialize gradient value\n",
    "total_grad = 0\n",
    "\n",
    "\n",
    "# function to compute gradient\n",
    "def compute_grad(w, feat_train, label_train):\n",
    "    \n",
    "    # initialize dimensions\n",
    "    n = len(feat_train)\n",
    "    d = len(w)\n",
    "    # initialize gradient vector\n",
    "    grad_sum = np.zeros(d)\n",
    "    \n",
    "    # compute gradient across all data\n",
    "    for i in range(n):\n",
    "        \n",
    "        x_i = feat_train[i]\n",
    "        y_i = label_train[i]\n",
    "        num = 1\n",
    "        denom = 1 + np.exp(-1 * np.dot(w, x_i))\n",
    "        to_add = ((num / denom) - y_i) * x_i\n",
    "        grad_sum += to_add\n",
    "    \n",
    "    # normalize gradient\n",
    "    grad = grad_sum / n\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# track gradient norms and number of iterations for plotting purposes\n",
    "gradient_norms = []\n",
    "num_iter = 0\n",
    "\n",
    "# do first step of gradient descent (since intial vector w of all zeros won't enter the loop)\n",
    "grad = compute_grad(w, feat_train, label_train)\n",
    "gradients.append(grad)\n",
    "w = w - gamma * grad\n",
    "\n",
    "# implement gradient descent to compute w\n",
    "while np.linalg.norm(grad) > epsilon:\n",
    "    grad = compute_grad(w, feat_train, label_train)\n",
    "    gradient_norms.append(np.linalg.norm(grad))\n",
    "    w = w - gamma * grad\n",
    "    num_iter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_iter = [i for i in range(num_iter)]\n",
    "plt.plot(x_iter, gradient_norms)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set display option for pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TNR</th>\n",
       "      <th>FNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.2339</td>\n",
       "      <td>0.7661</td>\n",
       "      <td>0.0102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.9490</td>\n",
       "      <td>0.1462</td>\n",
       "      <td>0.8538</td>\n",
       "      <td>0.0510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.0714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9082</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.0918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8980</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8776</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.9708</td>\n",
       "      <td>0.1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8469</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.1531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8265</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>0.1735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7959</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>0.2041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      t     TPR     FPR     TNR     FNR\n",
       "0   0.0  1.0000  1.0000  0.0000  0.0000\n",
       "1   0.1  0.9898  0.2339  0.7661  0.0102\n",
       "2   0.2  0.9490  0.1462  0.8538  0.0510\n",
       "3   0.3  0.9286  0.1111  0.8889  0.0714\n",
       "4   0.4  0.9082  0.0526  0.9474  0.0918\n",
       "5   0.5  0.8980  0.0351  0.9649  0.1020\n",
       "6   0.6  0.8776  0.0292  0.9708  0.1224\n",
       "7   0.7  0.8469  0.0175  0.9825  0.1531\n",
       "8   0.8  0.8265  0.0117  0.9883  0.1735\n",
       "9   0.9  0.7959  0.0058  0.9942  0.2041\n",
       "10  1.0  0.0000  0.0000  1.0000  1.0000"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define function to compute probability of a given observation\n",
    "def logistic_regression(w, x_i):\n",
    "    \"Return P(y = 1) given some weight vector w and observation x_i\"\n",
    "    prob = 1 / (1 + np.exp(-1 * np.dot(w, x_i)))\n",
    "    return prob\n",
    "\n",
    "# track rows of info to display into dataframe at end\n",
    "row_list = []\n",
    "\n",
    "# count total number of positives and negatives\n",
    "labels = np.unique(label_test, return_counts = True)\n",
    "counts = labels[1]\n",
    "total_neg = counts[0]\n",
    "total_pos = counts[1]\n",
    "\n",
    "# initialize list of thresholds\n",
    "thresholds = [i/10 for i in range(11)]\n",
    "\n",
    "# repeat tp/fp calculation for each threshold\n",
    "for thrs in thresholds:\n",
    "    tp, fp = 0, 0\n",
    "    # calculate a probability for each observation\n",
    "    for i, obs in enumerate(feat_test):\n",
    "        # compute probability\n",
    "        prob = logistic_regression(w, obs)\n",
    "        # assign predicted label\n",
    "        y_pred = 1 if prob > thrs else 0\n",
    "        # assign true label label\n",
    "        y_true = label_test[i]\n",
    "        \n",
    "        # track true positives and false positives\n",
    "        if y_pred == 1 and y_true == 1:\n",
    "            tp += 1\n",
    "        \n",
    "        elif y_pred == 1 and y_true == 0:\n",
    "            fp += 1\n",
    "    \n",
    "    # compute tpr/fpr/tnr/fnr\n",
    "    tpr = tp / total_pos\n",
    "    fpr = fp / total_neg\n",
    "    tnr = 1 - fpr\n",
    "    fnr = 1 - tpr\n",
    "    \n",
    "    # assemble row entry and add to row list\n",
    "    row = {'t': thrs, 'TPR': tpr, 'FPR': fpr, 'TNR': tnr, 'FNR': fnr}\n",
    "    row_list.append(row)\n",
    "\n",
    "# create table with row list, display\n",
    "df_result = pd.DataFrame(row_list)\n",
    "df_result\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
